#!/usr/bin/env python3
"""
Bitbucket Sequential Merge Analysis

A script that analyzes merge commits in sequence, comparing each merge commit
with the previous merge commit to track all changes between integration points.
Uses Bitbucket Server API v1.0 to analyze file-by-file differences.

Usage:
    python bitbucket_sequential_analysis.py --repo PROJECT/REPO [options]

Args:
    --repo: Repository in format PROJECT/REPO
    --branch: Branch to analyze (default: develop)
    --server: Bitbucket server URL (default: http://localhost:7990)
    --username: Bitbucket username for Basic Auth
    --password: Bitbucket password
    --token: Bitbucket access token (alternative to username/password)
    --output: Output directory for results (default: current directory)
    --from-commit: Starting commit ID (optional)
    --to-commit: Ending commit ID (optional)
    --verbose: Show detailed progress information
"""

import argparse
import csv
import json
import os
import requests
import sys
import time
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from requests.auth import HTTPBasicAuth


class BitbucketSequentialAnalyzer:
    def __init__(self, server, repo, branch='develop', auth=None, token=None, verbose=False):
        """Initialize the analyzer with connection parameters."""
        self.server = server.rstrip('/')
        self.repo = repo
        self.branch = branch
        self.auth = auth
        self.token = token
        self.verbose = verbose
        self.headers = {'Accept': 'application/json'}
        
        if token:
            self.headers['Authorization'] = f'Bearer {token}'
        
        # Parse repository information
        try:
            self.project, self.repo_slug = repo.split('/')
        except ValueError:
            raise ValueError("Repository should be in the format PROJECT/REPO")
        
        # Build API base URL
        self.api_base = f"{self.server}/rest/api/1.0/projects/{self.project}/repos/{self.repo_slug}"
        
        # Initialize result containers
        self.author_stats = defaultdict(lambda: {
            'merge_commits': 0,
            'files_added': 0,
            'files_modified': 0,
            'files_deleted': 0,
            'lines_added': 0,
            'lines_removed': 0
        })
        
        # Known binary file extensions
        self.binary_extensions = {
            'png', 'jpg', 'jpeg', 'gif', 'bmp', 'ico', 'pdf', 'zip', 'gz', 'tar',
            'jar', 'war', 'ear', 'class', 'exe', 'dll', 'pdb', 'bin', 'obj', 'so',
            'pyc', 'pyd', 'mp3', 'mp4', 'avi', 'mov', 'wmv', 'flv', 'woff', 'woff2',
            'ttf', 'eot', 'svg', 'psd', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx'
        }
    
    def log(self, message):
        """Print log message if verbose mode is enabled."""
        if self.verbose:
            print(message)
            
    def make_request(self, url, method='GET', params=None, data=None):
        """Make an HTTP request to the Bitbucket API with error handling."""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                response = requests.request(
                    method,
                    url,
                    params=params,
                    json=data,
                    headers=self.headers,
                    auth=self.auth,
                    timeout=30  # Add timeout for safety
                )
                response.raise_for_status()
                return response.json()
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)  # Exponential backoff
                    self.log(f"Request failed, retrying in {wait_time}s: {e}")
                    time.sleep(wait_time)
                else:
                    print(f"Error: API request failed after {max_retries} attempts.")
                    if hasattr(e, 'response') and e.response is not None:
                        print(f"Status code: {e.response.status_code}")
                        print(f"Response body: {e.response.text[:500]}")
                    sys.exit(1)
    
    def get_all_commits(self, from_commit=None, to_commit=None):
        """
        Retrieve all commits for the specified branch.
        
        Args:
            from_commit: Starting commit ID (optional)
            to_commit: Ending commit ID (optional)
            
        Returns:
            list: All commit objects
        """
        commits = []
        start = 0
        limit = 100  # Max per page
        
        # Build request parameters
        params = {
            'limit': limit,
            'start': start
        }
        
        if to_commit:
            params['until'] = to_commit
        else:
            params['until'] = self.branch
            
        if from_commit:
            params['since'] = from_commit
            
        print(f"Fetching commits for {self.repo} (branch: {self.branch})...")
        
        while True:
            url = f"{self.api_base}/commits"
            response = self.make_request(url, params=params)
            
            if not response.get('values'):
                break
                
            commits.extend(response['values'])
            
            if response.get('isLastPage', True):
                break
                
            start = response.get('nextPageStart')
            params['start'] = start
            
        # Sort commits chronologically
        commits.sort(key=lambda x: x.get('authorTimestamp', 0))
        
        print(f"Found {len(commits)} total commits")
        return commits
    
    def get_initial_commit(self, commits):
        """
        Find the initial commit of the repository.
        
        Args:
            commits: List of commit objects
            
        Returns:
            dict: Initial commit object or None
        """
        if not commits:
            return None
            
        # Sort by timestamp to find the earliest commit
        sorted_commits = sorted(commits, key=lambda x: x.get('authorTimestamp', 0))
        return sorted_commits[0]
    
    def get_merge_commits(self, commits):
        """
        Find all merge commits in the repository.
        
        Args:
            commits: List of commit objects
            
        Returns:
            list: List of merge commit objects
        """
        # Merge commits have more than one parent
        merge_commits = [commit for commit in commits if len(commit.get('parents', [])) > 1]
        print(f"Found {len(merge_commits)} merge commits")
        return merge_commits
    
    def get_significant_commits(self, commits):
        """
        Get initial commit and all merge commits, sorted chronologically.
        
        Args:
            commits: List of all commit objects
            
        Returns:
            list: List of significant commits (initial + merge commits)
        """
        significant_commits = []
        
        # Get initial commit
        initial_commit = self.get_initial_commit(commits)
        if initial_commit:
            significant_commits.append(initial_commit)
            print(f"Found initial commit: {initial_commit['id'][:8]}")
        
        # Get merge commits
        merge_commits = self.get_merge_commits(commits)
        if merge_commits:
            significant_commits.extend(merge_commits)
        
        # Sort chronologically
        significant_commits.sort(key=lambda x: x.get('authorTimestamp', 0))
        
        print(f"Total significant commits: {len(significant_commits)}")
        return significant_commits
    
    def get_compare_changes(self, from_commit, to_commit):
        """
        Get file changes between two commits using the compare endpoint.
        
        Args:
            from_commit: Source commit ID
            to_commit: Target commit ID
            
        Returns:
            list: List of file changes
        """
        url = f"{self.api_base}/compare/changes"
        params = {
            'from': from_commit,
            'to': to_commit,
            'limit': 1000
        }
        
        all_changes = []
        start = 0
        
        while True:
            params['start'] = start
            try:
                response = self.make_request(url, params=params)
                
                if not response.get('values'):
                    break
                    
                all_changes.extend(response.get('values', []))
                
                if response.get('isLastPage', True):
                    break
                    
                start = response.get('nextPageStart')
            except Exception as e:
                print(f"Error getting changes between {from_commit[:8]} and {to_commit[:8]}: {e}")
                return []
        
        # Convert to our format
        files = []
        for change in all_changes:
            path = change.get('path', {}).get('toString')
            if path:
                files.append({
                    'path': path,
                    'type': change.get('type')  # ADD, MODIFY, DELETE
                })
                
        return files
    
    def get_file_diff(self, from_commit, to_commit, file_path):
        """
        Get the diff for a specific file between two commits.
        
        Args:
            from_commit: Source commit ID
            to_commit: Target commit ID
            file_path: Path to the file
            
        Returns:
            dict: Diff information
        """
        try:
            # Use the compare/diff endpoint for comparing two commits
            url = f"{self.api_base}/compare/diff"
            params = {
                'from': from_commit,
                'to': to_commit,
                'path': file_path
            }
            
            return self.make_request(url, params=params)
        except requests.exceptions.HTTPError as e:
            if hasattr(e, 'response') and e.response.status_code == 404:
                # File not found or binary file
                self.log(f"Could not get diff for file {file_path}")
                return None
            raise
    
    def analyze_file_diff(self, diff):
        """
        Analyze a file diff to extract LOC statistics.
        
        Args:
            diff: Diff information from API
            
        Returns:
            dict: Line statistics
        """
        stats = {
            'lines_added': 0,
            'lines_removed': 0
        }
        
        if not diff:
            return stats
            
        # Process each diff in the response
        for diff_item in diff.get('diffs', []):
            # Process each hunk in the diff
            for hunk in diff_item.get('hunks', []):
                # Process each segment in the hunk
                for segment in hunk.get('segments', []):
                    segment_type = segment.get('type')
                    lines = segment.get('lines', [])
                    
                    if segment_type == 'ADDED':
                        stats['lines_added'] += len(lines)
                    elif segment_type == 'REMOVED':
                        stats['lines_removed'] += len(lines)
                    # CONTEXT segments are unchanged lines
        
        return stats
    
    def is_binary_file(self, file_path):
        """Check if a file is likely binary based on extension."""
        extension = Path(file_path).suffix.lower()[1:] if Path(file_path).suffix else ''
        return extension in self.binary_extensions
        
    def analyze_file_change(self, file_info, from_commit, to_commit):
        """
        Analyze changes to a specific file between two commits.
        
        Args:
            file_info: File information (path and type)
            from_commit: Source commit ID
            to_commit: Target commit ID
            
        Returns:
            dict: File analysis results
        """
        path = file_info['path']
        file_type = file_info['type']
        
        # Initialize result
        result = {
            'path': path,
            'type': file_type,
            'lines_added': 0,
            'lines_removed': 0
        }
        
        # For binary files, use simple statistics
        if self.is_binary_file(path):
            if file_type == 'ADD':
                result['lines_added'] = 1
            elif file_type == 'DELETE':
                result['lines_removed'] = 1
        else:
            # For text files, get the diff between commits
            diff = self.get_file_diff(from_commit, to_commit, path)
            if diff:
                stats = self.analyze_file_diff(diff)
                result['lines_added'] = stats['lines_added']
                result['lines_removed'] = stats['lines_removed']
        
        return result
    
    def analyze_merge_sequence(self):
        """
        Analyze all merge commits in sequence, comparing each with the previous one.
        
        Returns:
            dict: Complete analysis results
        """
        # Get all commits
        all_commits = self.get_all_commits()
        if not all_commits:
            print("No commits found to analyze.")
            sys.exit(1)
        
        # Get significant commits (initial + merge)
        significant_commits = self.get_significant_commits(all_commits)
        if len(significant_commits) <= 1:
            print("Not enough significant commits to analyze sequences.")
            sys.exit(1)
        
        # Initialize results
        results = {
            'repository': self.repo,
            'branch': self.branch,
            'analysis_date': datetime.now().isoformat(),
            'total_commits': len(all_commits),
            'significant_commits': len(significant_commits),
            'merge_sequences': []
        }
        
        # Analyze each consecutive pair of significant commits
        for i in range(1, len(significant_commits)):
            prev_commit = significant_commits[i-1]
            curr_commit = significant_commits[i]
            
            prev_id = prev_commit['id']
            curr_id = curr_commit['id']
            
            is_merge = len(curr_commit.get('parents', [])) > 1
            if not is_merge:
                continue  # Skip non-merge commits in the significant_commits list
                
            print(f"Analyzing merge {i}/{len(significant_commits)-1}: "
                  f"{prev_id[:8]} -> {curr_id[:8]}")
            
            # Get author information
            author_name = curr_commit.get('author', {}).get('name', 'Unknown')
            author_email = curr_commit.get('author', {}).get('emailAddress', 'Unknown')
            
            # Initialize sequence analysis
            sequence_analysis = {
                'from_commit': {
                    'id': prev_id,
                    'short_id': prev_id[:8],
                    'date': datetime.fromtimestamp(prev_commit.get('authorTimestamp', 0) / 1000).isoformat()
                },
                'to_commit': {
                    'id': curr_id,
                    'short_id': curr_id[:8], 
                    'date': datetime.fromtimestamp(curr_commit.get('authorTimestamp', 0) / 1000).isoformat(),
                    'message': curr_commit.get('message', ''),
                    'author': {
                        'name': author_name,
                        'email': author_email
                    }
                },
                'files': {
                    'added': 0,
                    'modified': 0,
                    'deleted': 0,
                    'total': 0
                },
                'lines': {
                    'added': 0,
                    'removed': 0,
                    'net_change': 0
                },
                'file_details': []
            }
            
            # Get all file changes between these commits
            changed_files = self.get_compare_changes(prev_id, curr_id)
            sequence_analysis['files']['total'] = len(changed_files)
            
            # Count files by type
            for file_info in changed_files:
                file_type = file_info['type']
                if file_type == 'ADD':
                    sequence_analysis['files']['added'] += 1
                elif file_type == 'MODIFY':
                    sequence_analysis['files']['modified'] += 1
                elif file_type == 'DELETE':
                    sequence_analysis['files']['deleted'] += 1
            
            # Analyze each file
            for file_info in changed_files:
                file_analysis = self.analyze_file_change(file_info, prev_id, curr_id)
                sequence_analysis['file_details'].append(file_analysis)
                
                # Update sequence statistics
                sequence_analysis['lines']['added'] += file_analysis['lines_added']
                sequence_analysis['lines']['removed'] += file_analysis['lines_removed']
            
            # Calculate net change
            sequence_analysis['lines']['net_change'] = (
                sequence_analysis['lines']['added'] - sequence_analysis['lines']['removed']
            )
            
            # Update author statistics
            self.author_stats[author_name]['merge_commits'] += 1
            self.author_stats[author_name]['files_added'] += sequence_analysis['files']['added']
            self.author_stats[author_name]['files_modified'] += sequence_analysis['files']['modified']
            self.author_stats[author_name]['files_deleted'] += sequence_analysis['files']['deleted']
            self.author_stats[author_name]['lines_added'] += sequence_analysis['lines']['added']
            self.author_stats[author_name]['lines_removed'] += sequence_analysis['lines']['removed']
            
            # Add sequence to results
            results['merge_sequences'].append(sequence_analysis)
        
        # Add author summary
        results['author_summary'] = dict(self.author_stats)
        
        # Calculate overall statistics
        total_stats = {
            'merge_commits': len(results['merge_sequences']),
            'files_added': sum(seq['files']['added'] for seq in results['merge_sequences']),
            'files_modified': sum(seq['files']['modified'] for seq in results['merge_sequences']),
            'files_deleted': sum(seq['files']['deleted'] for seq in results['merge_sequences']),
            'lines_added': sum(seq['lines']['added'] for seq in results['merge_sequences']),
            'lines_removed': sum(seq['lines']['removed'] for seq in results['merge_sequences']),
            'net_change': sum(seq['lines']['net_change'] for seq in results['merge_sequences'])
        }
        results['total_stats'] = total_stats
        
        return results
    
    def export_results(self, results, output_dir='.'):
        """
        Export analysis results to files.
        
        Args:
            results: Analysis results
            output_dir: Output directory
        """
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        repo_name = self.repo.replace('/', '_')
        
        # Export JSON results
        json_path = os.path.join(output_dir, f"{repo_name}_{timestamp}_sequence_analysis.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"Full analysis saved to: {json_path}")
        
        # Export author summary CSV
        authors_csv = os.path.join(output_dir, f"{repo_name}_{timestamp}_author_summary.csv")
        with open(authors_csv, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['name', 'merge_commits', 'files_added', 'files_modified', 
                         'files_deleted', 'lines_added', 'lines_removed', 'net_change']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for name, stats in results['author_summary'].items():
                # Calculate net change
                net_change = stats['lines_added'] - stats['lines_removed']
                
                # Write row
                writer.writerow({
                    'name': name,
                    'merge_commits': stats['merge_commits'],
                    'files_added': stats['files_added'],
                    'files_modified': stats['files_modified'],
                    'files_deleted': stats['files_deleted'],
                    'lines_added': stats['lines_added'],
                    'lines_removed': stats['lines_removed'],
                    'net_change': net_change
                })
        print(f"Author summary saved to: {authors_csv}")
        
        # Export merge sequence summary CSV
        sequences_csv = os.path.join(output_dir, f"{repo_name}_{timestamp}_merge_sequences.csv")
        with open(sequences_csv, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['from_commit', 'to_commit', 'author', 'date', 
                         'files_added', 'files_modified', 'files_deleted',
                         'lines_added', 'lines_removed', 'net_change', 'message']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for seq in results['merge_sequences']:
                # Write row
                writer.writerow({
                    'from_commit': seq['from_commit']['short_id'],
                    'to_commit': seq['to_commit']['short_id'],
                    'author': seq['to_commit']['author']['name'],
                    'date': seq['to_commit']['date'],
                    'files_added': seq['files']['added'],
                    'files_modified': seq['files']['modified'],
                    'files_deleted': seq['files']['deleted'],
                    'lines_added': seq['lines']['added'],
                    'lines_removed': seq['lines']['removed'],
                    'net_change': seq['lines']['net_change'],
                    'message': seq['to_commit']['message'][:80]  # Truncate long messages
                })
        print(f"Merge sequence summary saved to: {sequences_csv}")


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Analyze merge commits sequentially in a Bitbucket repository')
    
    parser.add_argument('--repo', required=True, help='Repository in format PROJECT/REPO')
    parser.add_argument('--branch', default='develop', help='Branch to analyze (default: develop)')
    parser.add_argument('--server', default='http://localhost:7990', help='Bitbucket server URL')
    parser.add_argument('--username', help='Bitbucket username')
    parser.add_argument('--password', help='Bitbucket password')
    parser.add_argument('--token', help='Bitbucket access token')
    parser.add_argument('--output', default='.', help='Output directory for results')
    parser.add_argument('--verbose', action='store_true', help='Show detailed progress info')
    
    return parser.parse_args()


def main():
    """Main execution function."""
    args = parse_arguments()
    
    # Set up authentication
    auth = None
    if args.username:
        password = args.password or os.environ.get('BITBUCKET_PASSWORD')
        if not password:
            print("Password not provided. Use --password or set BITBUCKET_PASSWORD environment variable")
            sys.exit(1)
        auth = HTTPBasicAuth(args.username, password)
    
    # Create analyzer
    analyzer = BitbucketSequentialAnalyzer(
        server=args.server,
        repo=args.repo,
        branch=args.branch,
        auth=auth,
        token=args.token,
        verbose=args.verbose
    )
    
    # Run analysis
    results = analyzer.analyze_merge_sequence()
    
    # Export results
    analyzer.export_results(results, args.output)
    
    # Print summary statistics
    total = results['total_stats']
    print("\n=== Analysis Summary ===")
    print(f"Repository: {results['repository']}")
    print(f"Branch: {results['branch']}")
    print(f"Total commits: {results['total_commits']}")
    print(f"Significant commits: {results['significant_commits']}")
    print(f"Merge sequences analyzed: {total['merge_commits']}")
    print(f"Files:")
    print(f"  - Added: {total['files_added']}")
    print(f"  - Modified: {total['files_modified']}")
    print(f"  - Deleted: {total['files_deleted']}")
    print(f"Lines of code:")
    print(f"  - Added: {total['lines_added']}")
    print(f"  - Removed: {total['lines_removed']}")
    print(f"  - Net change: {total['net_change']}")
    
    print("\nAuthor Summary:")
    for name, stats in results['author_summary'].items():
        net_change = stats['lines_added'] - stats['lines_removed']
        print(f"  {name}:")
        print(f"    - Merge commits: {stats['merge_commits']}")
        print(f"    - Lines added: {stats['lines_added']}")
        print(f"    - Lines removed: {stats['lines_removed']}")
        print(f"    - Net change: {net_change}")


if __name__ == "__main__":
    main()
