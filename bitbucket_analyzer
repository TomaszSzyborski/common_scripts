#!/usr/bin/env python3
"""
Bitbucket Repository Analyzer

This script analyzes Bitbucket on-premise server repositories, focusing on initial 
and merge commits. It retrieves file lists for each commit, performs diffs between 
commits, and analyzes changes to categorize files as added, modified, deleted, or 
moved, along with calculating Lines of Code (LOC) metrics per author.
"""

import requests
import json
import argparse
import sys
import re
from collections import defaultdict
from datetime import datetime
import logging
import os
from urllib.parse import urljoin
import csv

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("bitbucket_analysis.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("BitbucketAnalyzer")

class BitbucketAnalyzer:
    def __init__(self, server_url, username, password, project_key, repo_slug, 
                 verify_ssl=True, output_dir="output"):
        """
        Initialize the BitbucketAnalyzer with server connection details.
        
        Args:
            server_url (str): Base URL of the Bitbucket server
            username (str): Username for authentication
            password (str): Password or token for authentication
            project_key (str): Bitbucket project key
            repo_slug (str): Repository slug
            verify_ssl (bool): Whether to verify SSL certificates
            output_dir (str): Directory to save output files
        """
        self.server_url = server_url.rstrip('/')
        self.auth = (username, password)
        self.project_key = project_key
        self.repo_slug = repo_slug
        self.verify_ssl = verify_ssl
        self.output_dir = output_dir
        
        # Base API URL for this repository
        self.api_base = f"{self.server_url}/rest/api/1.0/projects/{project_key}/repos/{repo_slug}"
        
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        # Store commit history
        self.commits = []
        self.commit_map = {}  # Map commit IDs to their data
        self.file_history = defaultdict(list)  # Track file changes across commits
        self.author_stats = defaultdict(lambda: {
            'commits': 0,
            'files_added': 0,
            'files_modified': 0,
            'files_deleted': 0,
            'files_moved': 0,
            'lines_added': 0,
            'lines_removed': 0
        })
        
        logger.info(f"Initialized BitbucketAnalyzer for {project_key}/{repo_slug}")

    def _make_request(self, endpoint, method="GET", params=None, data=None):
        """Make an API request to the Bitbucket server."""
        url = urljoin(self.api_base, endpoint)
        
        try:
            response = requests.request(
                method=method,
                url=url,
                auth=self.auth,
                params=params,
                json=data,
                verify=self.verify_ssl
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"API request failed: {e}")
            if hasattr(e.response, 'text'):
                logger.error(f"Response: {e.response.text}")
            raise

    def fetch_all_commits(self):
        """Fetch all commits from the repository."""
        logger.info("Fetching all commits...")
        
        all_commits = []
        start = 0
        limit = 100
        
        while True:
            params = {
                'start': start,
                'limit': limit
            }
            
            response = self._make_request('commits', params=params)
            values = response.get('values', [])
            
            if not values:
                break
                
            all_commits.extend(values)
            
            # Check if we need to fetch more
            is_last_page = response.get('isLastPage', True)
            if is_last_page:
                break
                
            start = response.get('nextPageStart')
        
        logger.info(f"Fetched {len(all_commits)} commits")
        return all_commits

    def identify_important_commits(self):
        """Identify initial commit and merge commits."""
        logger.info("Identifying initial and merge commits...")
        
        all_commits = self.fetch_all_commits()
        
        # Sort commits by date (oldest first)
        sorted_commits = sorted(all_commits, key=lambda c: c['authorTimestamp'])
        
        # Get the initial commit
        initial_commit = sorted_commits[0] if sorted_commits else None
        
        # Identify merge commits (those with multiple parents)
        merge_commits = [c for c in all_commits if len(c.get('parents', [])) > 1]
        
        # Create a list of commits to analyze
        important_commits = []
        if initial_commit:
            important_commits.append(initial_commit)
            logger.info(f"Initial commit: {initial_commit['id']} by {initial_commit['author']['name']}")
            
        for commit in merge_commits:
            important_commits.append(commit)
            logger.info(f"Merge commit: {commit['id']} by {commit['author']['name']}")
        
        # Add all important commits to the commit map
        for commit in important_commits:
            self.commit_map[commit['id']] = commit
            
        # Sort important commits by timestamp
        important_commits = sorted(important_commits, key=lambda c: c['authorTimestamp'])
        self.commits = important_commits
        
        return important_commits

    def get_commit_files(self, commit_id):
        """Get list of files in a specific commit."""
        logger.info(f"Getting files for commit {commit_id}")
        
        endpoint = f"commits/{commit_id}/changes"
        params = {'limit': 1000}  # Adjust if you have more files per commit
        
        try:
            response = self._make_request(endpoint, params=params)
            return response.get('values', [])
        except Exception as e:
            logger.error(f"Failed to get files for commit {commit_id}: {e}")
            return []

    def get_file_diff(self, from_commit, to_commit, file_path):
        """Get the diff for a specific file between two commits."""
        logger.info(f"Getting diff for {file_path} between {from_commit} and {to_commit}")
        
        endpoint = "compare/diff"
        params = {
            'from': from_commit,
            'to': to_commit,
            'path': file_path
        }
        
        try:
            response = requests.get(
                f"{self.api_base}/{endpoint}",
                auth=self.auth,
                params=params,
                verify=self.verify_ssl
            )
            # Diff is returned as text, not JSON
            return response.text
        except Exception as e:
            logger.error(f"Failed to get diff: {e}")
            return ""

    def parse_diff(self, diff_text):
        """
        Parse diff text to extract lines added/removed.
        
        Returns:
            tuple: (lines_added, lines_removed)
        """
        if not diff_text:
            return 0, 0
            
        lines_added = 0
        lines_removed = 0
        
        for line in diff_text.split('\n'):
            if line.startswith('+') and not line.startswith('+++'):
                lines_added += 1
            elif line.startswith('-') and not line.startswith('---'):
                lines_removed += 1
                
        return lines_added, lines_removed

    def analyze_commit(self, commit, previous_commit=None):
        """
        Analyze changes in a commit compared to previous commit.
        
        Args:
            commit: Current commit data
            previous_commit: Previous commit for comparison, or None for initial commit
            
        Returns:
            dict: Analysis results for this commit
        """
        commit_id = commit['id']
        commit_files = self.get_commit_files(commit_id)
        
        # Initialize stats for this commit
        commit_stats = {
            'id': commit_id,
            'author': commit['author']['name'],
            'email': commit['author']['emailAddress'],
            'timestamp': commit['authorTimestamp'],
            'date': datetime.fromtimestamp(commit['authorTimestamp'] / 1000).strftime('%Y-%m-%d %H:%M:%S'),
            'message': commit['message'],
            'files_added': [],
            'files_modified': [],
            'files_deleted': [],
            'files_moved': [],
            'lines_added': 0,
            'lines_removed': 0
        }
        
        # Update author stats
        author = commit['author']['name']
        self.author_stats[author]['commits'] += 1
        
        # Get previous files if we have a previous commit
        previous_files = set()
        if previous_commit:
            previous_files_data = self.get_commit_files(previous_commit['id'])
            previous_files = {f['path'] for f in previous_files_data}
        
        # Current files
        current_files = {f['path'] for f in commit_files}
        
        # Detect added/deleted files
        added_files = current_files - previous_files if previous_commit else current_files
        deleted_files = previous_files - current_files if previous_commit else set()
        
        # Detect changed files (need to check diff)
        changed_files = set()
        moved_files = set()
        
        # Process diffs for each file
        for file_data in commit_files:
            file_path = file_data['path']
            
            # Skip if file was just added
            if file_path in added_files:
                commit_stats['files_added'].append(file_path)
                self.author_stats[author]['files_added'] += 1
                continue
                
            # For existing files, get diff with previous commit
            if previous_commit:
                diff_text = self.get_file_diff(previous_commit['id'], commit_id, file_path)
                lines_added, lines_removed = self.parse_diff(diff_text)
                
                # Update LOC stats
                commit_stats['lines_added'] += lines_added
                commit_stats['lines_removed'] += lines_removed
                self.author_stats[author]['lines_added'] += lines_added
                self.author_stats[author]['lines_removed'] += lines_removed
                
                # If file was changed but not added/deleted, it was modified
                if file_path not in added_files and file_path not in deleted_files:
                    # Check if it might be a move/rename
                    # Simple heuristic: similar content but different path
                    possible_moved_file = False
                    # TODO: Implement a better heuristic for moved files
                    
                    if possible_moved_file:
                        commit_stats['files_moved'].append(file_path)
                        self.author_stats[author]['files_moved'] += 1
                    else:
                        commit_stats['files_modified'].append(file_path)
                        self.author_stats[author]['files_modified'] += 1
            
        # Add deleted files to stats
        for file_path in deleted_files:
            commit_stats['files_deleted'].append(file_path)
            self.author_stats[author]['files_deleted'] += 1
            
        logger.info(f"Commit {commit_id} analysis: {len(commit_stats['files_added'])} added, "
                   f"{len(commit_stats['files_modified'])} modified, "
                   f"{len(commit_stats['files_deleted'])} deleted, "
                   f"{len(commit_stats['files_moved'])} moved, "
                   f"{commit_stats['lines_added']} lines added, "
                   f"{commit_stats['lines_removed']} lines removed")
                   
        return commit_stats

    def analyze_repository(self):
        """Perform full repository analysis on initial and merge commits."""
        logger.info("Starting repository analysis...")
        
        # Get important commits
        important_commits = self.identify_important_commits()
        if not important_commits:
            logger.error("No commits found to analyze")
            return False
            
        # Analysis results
        analysis_results = []
        
        # Analyze each commit
        previous_commit = None
        for commit in important_commits:
            logger.info(f"Analyzing commit {commit['id']} by {commit['author']['name']}")
            
            # Analyze this commit
            commit_stats = self.analyze_commit(commit, previous_commit)
            analysis_results.append(commit_stats)
            
            # Update previous commit for next iteration
            previous_commit = commit
            
        # Save results
        self.save_results(analysis_results)
        
        logger.info("Repository analysis completed")
        return True
        
    def save_results(self, analysis_results):
        """Save analysis results to files."""
        # Save commit analysis as JSON
        commits_file = os.path.join(self.output_dir, f"{self.project_key}_{self.repo_slug}_commits.json")
        with open(commits_file, 'w') as f:
            json.dump(analysis_results, f, indent=2)
            
        # Save author stats as CSV
        authors_file = os.path.join(self.output_dir, f"{self.project_key}_{self.repo_slug}_authors.csv")
        with open(authors_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Author', 'Email', 'Commits', 'Files Added', 'Files Modified', 
                            'Files Deleted', 'Files Moved', 'Lines Added', 'Lines Removed', 'Net LOC'])
            
            for commit in analysis_results:
                author = commit['author']
                email = commit['email']
                # Check if we already have a row for this author
                if not any(row[0] == author for row in csv.reader(open(authors_file))):
                    stats = self.author_stats[author]
                    net_loc = stats['lines_added'] - stats['lines_removed']
                    writer.writerow([
                        author, email, stats['commits'], stats['files_added'], 
                        stats['files_modified'], stats['files_deleted'], stats['files_moved'],
                        stats['lines_added'], stats['lines_removed'], net_loc
                    ])
                    
        logger.info(f"Results saved to {self.output_dir} directory")
        

def main():
    """Main function to run the Bitbucket repository analyzer."""
    parser = argparse.ArgumentParser(description='Analyze Bitbucket repository commits')
    
    parser.add_argument('--server', required=True, help='Bitbucket server URL')
    parser.add_argument('--username', required=True, help='Bitbucket username')
    parser.add_argument('--password', required=True, help='Bitbucket password or token')
    parser.add_argument('--project', required=True, help='Bitbucket project key')
    parser.add_argument('--repo', required=True, help='Repository slug')
    parser.add_argument('--no-verify-ssl', action='store_true', help='Disable SSL verification')
    parser.add_argument('--output-dir', default='output', help='Output directory')
    
    args = parser.parse_args()
    
    try:
        analyzer = BitbucketAnalyzer(
            server_url=args.server,
            username=args.username,
            password=args.password,
            project_key=args.project,
            repo_slug=args.repo,
            verify_ssl=not args.no_verify_ssl,
            output_dir=args.output_dir
        )
        
        success = analyzer.analyze_repository()
        
        if success:
            print(f"Analysis completed successfully. Results saved to {args.output_dir} directory.")
            return 0
        else:
            print("Analysis failed. Check the logs for more information.")
            return 1
            
    except Exception as e:
        logger.error(f"Error during analysis: {e}", exc_info=True)
        print(f"Error: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
