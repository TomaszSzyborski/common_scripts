import requests
import json
from collections import defaultdict
from datetime import datetime
import argparse
import logging


class BitbucketAnalyzer:
    """
    A class to analyze Bitbucket repositories, focusing on merge commits
    to track file changes and author contributions.
    """
    
    def __init__(self, server_url, project_key, repository_slug, auth=None):
        """
        Initialize the BitbucketAnalyzer with server and repo information.
        
        Args:
            server_url (str): The Bitbucket server URL
            project_key (str): The project key
            repository_slug (str): The repository slug
            auth (tuple, optional): Basic auth tuple (username, password/token)
        """
        self.server_url = server_url.rstrip('/')
        self.project_key = project_key
        self.repository_slug = repository_slug
        self.auth = auth
        self.base_url = f"{self.server_url}/rest/api/latest/projects/{self.project_key}/repos/{self.repository_slug}"
        self.session = requests.Session()
        
        if auth:
            self.session.auth = auth
            
        # Data tracking
        self.authors = defaultdict(lambda: {
            'commits': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'files_added': 0,
            'files_removed': 0,
            'files_modified': 0
        })
        self.file_history = {}
        
        # Set up logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger('BitbucketAnalyzer')
    
    def get_commits(self):
        """
        Get all commits from the repository, focusing on initial commit and merge commits.
        
        Returns:
            list: A list of commit objects
        """
        self.logger.info(f"Fetching commits for {self.project_key}/{self.repository_slug}")
        
        # First, get the initial commit
        initial_commit = self._get_initial_commit()
        if not initial_commit:
            self.logger.error("Could not find initial commit")
            return []
            
        # Then get all merge commits
        merge_commits = self._get_merge_commits()
        
        # Create the final ordered list starting with initial commit
        commits = [initial_commit] + merge_commits
        
        self.logger.info(f"Found {len(commits)} commits (initial + {len(merge_commits)} merge commits)")
        return commits
    
    def _get_initial_commit(self):
        """
        Get the initial commit of the repository.
        
        Returns:
            dict: The initial commit object
        """
        url = f"{self.base_url}/commits"
        params = {
            'until': 'master',
            'order': 'OLDEST',
            'limit': 1
        }
        
        response = self.session.get(url, params=params)
        
        if response.status_code == 200:
            data = response.json()
            if data.get('values') and len(data['values']) > 0:
                return data['values'][0]
        
        self.logger.error(f"Error fetching initial commit: {response.status_code}")
        return None
    
    def _get_merge_commits(self):
        """
        Get all merge commits in the repository.
        
        Returns:
            list: A list of merge commit objects
        """
        merge_commits = []
        url = f"{self.base_url}/commits"
        params = {
            'merges': 'only',
            'limit': 100,
            'start': 0
        }
        
        while True:
            response = self.session.get(url, params=params)
            
            if response.status_code != 200:
                self.logger.error(f"Error fetching merge commits: {response.status_code}")
                break
            
            data = response.json()
            values = data.get('values', [])
            
            if not values:
                break
                
            merge_commits.extend(values)
            
            # Check if there are more commits to fetch
            is_last_page = data.get('isLastPage', True)
            if is_last_page:
                break
                
            params['start'] = data.get('nextPageStart', 0)
        
        # Sort commits by date (oldest first)
        merge_commits.sort(key=lambda c: datetime.strptime(c.get('authorTimestamp', 0) / 1000, '%Y-%m-%d %H:%M:%S'))
        
        return merge_commits
    
    def analyze_changes(self, commits):
        """
        Analyze changes between successive commits and gather statistics.
        
        Args:
            commits (list): List of commit objects to analyze
        """
        self.logger.info("Analyzing changes between commits")
        
        # Process each commit in order
        for i, current_commit in enumerate(commits):
            # Skip the first commit (it doesn't have a parent to compare to)
            if i == 0:
                self._process_initial_commit(current_commit)
                continue
                
            # Get the previous commit to compare against
            previous_commit = commits[i-1]
            
            self._analyze_commit_changes(current_commit, previous_commit)
    
    def _process_initial_commit(self, commit):
        """
        Process the initial commit, tracking all files as "added".
        
        Args:
            commit (dict): The initial commit object
        """
        commit_id = commit.get('id')
        author = commit.get('author', {}).get('displayName', 'Unknown')
        self.logger.info(f"Processing initial commit: {commit_id[:8]} by {author}")
        
        # Fetch the changes for the initial commit
        url = f"{self.base_url}/commits/{commit_id}/changes"
        params = {'limit': 1000}
        
        response = self.session.get(url, params=params)
        
        if response.status_code != 200:
            self.logger.error(f"Error fetching changes for initial commit: {response.status_code}")
            return
            
        data = response.json()
        changes = data.get('values', [])
        
        # Process all files in the initial commit as "added"
        for change in changes:
            path = change.get('path', {}).get('toString', '')
            content_id = change.get('contentId', '')
            
            # Track the file
            self.file_history[path] = {
                'commit_id': commit_id,
                'author': author,
                'content_id': content_id,
                'status': 'ADDED'
            }
            
            # Update author stats
            self.authors[author]['commits'] += 1
            self.authors[author]['files_added'] += 1
            
            # Update line counts if available
            if 'size' in change:
                self.authors[author]['lines_added'] += change.get('size', 0)
    
    def _analyze_commit_changes(self, current_commit, previous_commit):
        """
        Analyze changes between two commits.
        
        Args:
            current_commit (dict): The current commit object
            previous_commit (dict): The previous commit object to compare against
        """
        current_id = current_commit.get('id')
        previous_id = previous_commit.get('id')
        author = current_commit.get('author', {}).get('displayName', 'Unknown')
        
        self.logger.info(f"Analyzing changes: {previous_id[:8]} â†’ {current_id[:8]} by {author}")
        
        # Fetch the changes between the two commits
        url = f"{self.base_url}/commits/{current_id}/changes"
        params = {
            'since': previous_id,
            'start': 0,
            'limit': 1000
        }
        
        response = self.session.get(url, params=params)
        
        if response.status_code != 200:
            self.logger.error(f"Error fetching changes between commits: {response.status_code}")
            return
            
        data = response.json()
        changes = data.get('values', [])
        
        # Process all changes
        for change in changes:
            path = change.get('path', {}).get('toString', '')
            content_id = change.get('contentId', '')
            node_type = change.get('nodeType', '')
            type_of_change = change.get('type', '')
            
            # Update author stats
            self.authors[author]['commits'] = self.authors[author]['commits'] + 1 if i == 0 else self.authors[author]['commits']
            
            # Track file changes
            if type_of_change == 'ADD':
                self.file_history[path] = {
                    'commit_id': current_id,
                    'author': author,
                    'content_id': content_id,
                    'status': 'ADDED'
                }
                self.authors[author]['files_added'] += 1
                self.authors[author]['lines_added'] += change.get('size', 0)
                
            elif type_of_change == 'DELETE':
                if path in self.file_history:
                    self.file_history[path]['status'] = 'DELETED'
                    self.file_history[path]['deleted_by'] = author
                    self.file_history[path]['deleted_in'] = current_id
                
                self.authors[author]['files_removed'] += 1
                self.authors[author]['lines_removed'] += change.get('size', 0)
                
            elif type_of_change == 'MOVE':
                source_path = change.get('srcPath', {}).get('toString', '')
                
                # Update the file history for the source path
                if source_path in self.file_history:
                    self.file_history[source_path]['status'] = 'MOVED'
                    self.file_history[source_path]['moved_to'] = path
                
                # Add the new path
                self.file_history[path] = {
                    'commit_id': current_id,
                    'author': author,
                    'content_id': content_id,
                    'status': 'ADDED',
                    'moved_from': source_path
                }
                
                # This is considered a modification
                self.authors[author]['files_modified'] += 1
                
            elif type_of_change == 'MODIFY':
                # Update existing file stats
                if path in self.file_history:
                    old_content_id = self.file_history[path].get('content_id', '')
                    
                    # Only count as modified if content actually changed
                    if old_content_id != content_id:
                        self.file_history[path]['content_id'] = content_id
                        self.file_history[path]['last_modified_by'] = author
                        self.file_history[path]['last_modified_in'] = current_id
                        
                        # Update line count stats
                        self.authors[author]['files_modified'] += 1
                        
                        # For modifications, we need to get the specific line changes
                        self._update_line_changes(current_id, path, author)
    
    def _update_line_changes(self, commit_id, path, author):
        """
        Get detailed line changes for a modified file.
        
        Args:
            commit_id (str): The commit ID
            path (str): The file path
            author (str): The author's name
        """
        url = f"{self.base_url}/commits/{commit_id}/diff/{path}"
        
        response = self.session.get(url)
        
        if response.status_code != 200:
            self.logger.warning(f"Could not get diff for {path} in commit {commit_id[:8]}")
            return
            
        diff_content = response.text
        
        # Parse the diff to get line changes
        lines_added = 0
        lines_removed = 0
        
        for line in diff_content.split('\n'):
            if line.startswith('+') and not line.startswith('+++'):
                lines_added += 1
            elif line.startswith('-') and not line.startswith('---'):
                lines_removed += 1
        
        # Update author stats
        self.authors[author]['lines_added'] += lines_added
        self.authors[author]['lines_removed'] += lines_removed
    
    def generate_report(self):
        """
        Generate a report of author contributions and file statistics.
        
        Returns:
            dict: Report data
        """
        self.logger.info("Generating final report")
        
        # Calculate current file counts
        current_files = {}
        for path, info in self.file_history.items():
            if info.get('status') not in ['DELETED', 'MOVED']:
                current_files[path] = info
        
        # Prepare the report
        report = {
            'repository': f"{self.project_key}/{self.repository_slug}",
            'analysis_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'author_stats': dict(self.authors),
            'file_counts': {
                'total_files_ever': len(self.file_history),
                'current_files': len(current_files),
                'deleted_files': sum(1 for info in self.file_history.values() if info.get('status') == 'DELETED'),
                'moved_files': sum(1 for info in self.file_history.values() if info.get('status') == 'MOVED')
            },
            'file_types': self._get_file_type_stats(current_files)
        }
        
        return report
    
    def _get_file_type_stats(self, files):
        """
        Get statistics about file types in the repository.
        
        Args:
            files (dict): Dictionary of current files
            
        Returns:
            dict: File type statistics
        """
        file_types = defaultdict(int)
        
        for path in files:
            if '.' in path:
                extension = path.split('.')[-1].lower()
                file_types[extension] += 1
            else:
                file_types['no_extension'] += 1
        
        return dict(file_types)


def main():
    """Main function to run the Bitbucket analyzer."""
    parser = argparse.ArgumentParser(description='Analyze Bitbucket repository changes and author contributions')
    
    parser.add_argument('--server', required=True, help='Bitbucket server URL')
    parser.add_argument('--project', required=True, help='Project key')
    parser.add_argument('--repo', required=True, help='Repository slug')
    parser.add_argument('--username', help='Bitbucket username')
    parser.add_argument('--password', help='Bitbucket password or token')
    parser.add_argument('--output', default='report.json', help='Output file name')
    
    args = parser.parse_args()
    
    # Set up authentication if provided
    auth = None
    if args.username and args.password:
        auth = (args.username, args.password)
    
    # Create the analyzer
    analyzer = BitbucketAnalyzer(
        server_url=args.server,
        project_key=args.project,
        repository_slug=args.repo,
        auth=auth
    )
    
    # Get commits and analyze
    commits = analyzer.get_commits()
    if commits:
        analyzer.analyze_changes(commits)
        report = analyzer.generate_report()
        
        # Save the report
        with open(args.output, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"Analysis complete! Report saved to {args.output}")
        
        # Print a summary to console
        print("\nSummary:")
        print(f"Repository: {report['repository']}")
        print(f"Total files tracked: {report['file_counts']['total_files_ever']}")
        print(f"Current files: {report['file_counts']['current_files']}")
        print("\nTop contributors:")
        
        # Sort authors by lines of code contributed
        sorted_authors = sorted(
            report['author_stats'].items(),
            key=lambda x: x[1]['lines_added'] + x[1]['lines_removed'],
            reverse=True
        )
        
        for author, stats in sorted_authors[:5]:  # Show top 5
            total_lines = stats['lines_added'] + stats['lines_removed']
            print(f"  {author}: {total_lines} lines ({stats['lines_added']} added, {stats['lines_removed']} removed)")


if __name__ == "__main__":
    main()
