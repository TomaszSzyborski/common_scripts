#!/usr/bin/env python3
"""
Bitbucket File-Based Change Analysis

A script that analyzes commit changes in a Bitbucket on-premise repository using file-by-file diff
operations. The script uses Bitbucket Server API v1.0 to extract detailed statistics about code
changes between commits, correctly handling the file diff operations.

Usage:
    python bitbucket_file_analysis.py --repo PROJECT/REPO [options]

Args:
    --repo: Repository in format PROJECT/REPO
    --branch: Branch to analyze (default: develop)
    --server: Bitbucket server URL (default: http://localhost:7990)
    --username: Bitbucket username for Basic Auth
    --password: Bitbucket password
    --token: Bitbucket access token (alternative to username/password)
    --output: Output directory for results (default: current directory)
    --from-commit: Starting commit ID (optional)
    --to-commit: Ending commit ID (optional)
    --only-significant: Analyze only initial and merge commits
    --verbose: Show detailed progress information
"""

import argparse
import csv
import json
import os
import requests
import sys
import time
from collections import defaultdict, Counter
from datetime import datetime
from pathlib import Path
from requests.auth import HTTPBasicAuth


class BitbucketAnalyzer:
    def __init__(self, server, repo, branch='develop', auth=None, token=None, verbose=False, only_significant=False):
        """Initialize the analyzer with connection parameters."""
        self.server = server.rstrip('/')
        self.repo = repo
        self.branch = branch
        self.auth = auth
        self.token = token
        self.verbose = verbose
        self.only_significant = only_significant
        self.headers = {'Accept': 'application/json'}
        
        if token:
            self.headers['Authorization'] = f'Bearer {token}'
        
        # Parse repository information
        try:
            self.project, self.repo_slug = repo.split('/')
        except ValueError:
            raise ValueError("Repository should be in the format PROJECT/REPO")
        
        # Build API base URL
        self.api_base = f"{self.server}/rest/api/1.0/projects/{self.project}/repos/{self.repo_slug}"
        
        # Initialize result containers
        self.commits_analyzed = 0
        self.total_stats = {
            'files_changed': 0,
            'files_added': 0,
            'files_modified': 0,
            'files_deleted': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        }
        self.file_stats = defaultdict(lambda: {
            'changes': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        })
        self.author_stats = defaultdict(lambda: {
            'commits': 0,
            'files_changed': 0,
            'files_added': 0,
            'files_modified': 0,
            'files_deleted': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        })
        self.extension_stats = defaultdict(lambda: {
            'files': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        })
        self.directory_stats = defaultdict(lambda: {
            'files': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        })
        
        # Known binary file extensions
        self.binary_extensions = {
            'png', 'jpg', 'jpeg', 'gif', 'bmp', 'ico', 'pdf', 'zip', 'gz', 'tar',
            'jar', 'war', 'ear', 'class', 'exe', 'dll', 'pdb', 'bin', 'obj', 'so',
            'pyc', 'pyd', 'mp3', 'mp4', 'avi', 'mov', 'wmv', 'flv', 'woff', 'woff2',
            'ttf', 'eot', 'svg', 'psd', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx'
        }
    
    def log(self, message):
        """Print log message if verbose mode is enabled."""
        if self.verbose:
            print(message)
            
    def make_request(self, url, method='GET', params=None, data=None):
        """Make an HTTP request to the Bitbucket API with error handling."""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                response = requests.request(
                    method,
                    url,
                    params=params,
                    json=data,
                    headers=self.headers,
                    auth=self.auth,
                    timeout=30  # Add timeout for safety
                )
                response.raise_for_status()
                return response.json()
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)  # Exponential backoff
                    self.log(f"Request failed, retrying in {wait_time}s: {e}")
                    time.sleep(wait_time)
                else:
                    print(f"Error: API request failed after {max_retries} attempts.")
                    if hasattr(e, 'response') and e.response is not None:
                        print(f"Status code: {e.response.status_code}")
                        print(f"Response body: {e.response.text[:500]}")
                    sys.exit(1)
    
    def get_all_commits(self, from_commit=None, to_commit=None):
        """
        Retrieve all commits for the specified branch.
        
        Args:
            from_commit: Starting commit ID (optional)
            to_commit: Ending commit ID (optional)
            
        Returns:
            list: All commit objects
        """
        commits = []
        start = 0
        limit = 100  # Max per page
        
        # Build request parameters
        params = {
            'limit': limit,
            'start': start
        }
        
        if to_commit:
            params['until'] = to_commit
        else:
            params['until'] = self.branch
            
        if from_commit:
            params['since'] = from_commit
            
        print(f"Fetching commits for {self.repo} (branch: {self.branch})...")
        
        while True:
            url = f"{self.api_base}/commits"
            response = self.make_request(url, params=params)
            
            if not response.get('values'):
                break
                
            commits.extend(response['values'])
            
            if response.get('isLastPage', True):
                break
                
            start = response.get('nextPageStart')
            params['start'] = start
            
        # Sort commits chronologically
        commits.sort(key=lambda x: x.get('authorTimestamp', 0))
        
        print(f"Found {len(commits)} commits")
        return commits
        
    def get_initial_commit(self, commits):
        """
        Find the initial commit of the repository.
        
        Args:
            commits: List of commit objects
            
        Returns:
            dict: Initial commit object or None
        """
        if not commits:
            return None
            
        # Sort by timestamp to find the earliest commit
        sorted_commits = sorted(commits, key=lambda x: x.get('authorTimestamp', 0))
        return sorted_commits[0]
        
    def get_merge_commits(self, commits):
        """
        Find all merge commits in the repository.
        
        Args:
            commits: List of commit objects
            
        Returns:
            list: List of merge commit objects
        """
        # Merge commits have more than one parent
        return [commit for commit in commits if len(commit.get('parents', [])) > 1]
        
    def filter_significant_commits(self, commits):
        """
        Filter commits to include only initial and merge commits, and
        establish commit history tracking to analyze changes between significant points.
        
        Args:
            commits: List of all commit objects
            
        Returns:
            list: A list containing tuples of (commit, previous_significant_commit) pairs
        """
        significant_commits = []
        previous_significant = None
        
        # First, get initial commit
        initial_commit = self.get_initial_commit(commits)
        if initial_commit:
            # Initial commit has no previous significant commit
            significant_commits.append((initial_commit, None))
            previous_significant = initial_commit
            print(f"Found initial commit: {initial_commit['id'][:8]}")
        
        # Then find all merge commits
        merge_commits = self.get_merge_commits(commits)
        if merge_commits:
            # Sort merge commits chronologically
            merge_commits.sort(key=lambda x: x.get('authorTimestamp', 0))
            print(f"Found {len(merge_commits)} merge commits")
            
            # Add each merge commit with its previous significant commit reference
            for merge_commit in merge_commits:
                significant_commits.append((merge_commit, previous_significant))
                previous_significant = merge_commit
        
        print(f"Total significant commits: {len(significant_commits)}")
        return significant_commits
    
    def get_commit_files(self, commit_id):
        """
        Get list of all files in a commit.
        
        Args:
            commit_id: Commit ID
            
        Returns:
            list: List of file paths
        """
        url = f"{self.api_base}/commits/{commit_id}"
        commit_data = self.make_request(url)
        
        # Fetch files modified in this commit
        changes_url = f"{self.api_base}/commits/{commit_id}/changes"
        params = {'limit': 1000}  # Large limit to get all files
        
        all_changes = []
        start = 0
        
        while True:
            params['start'] = start
            changes_data = self.make_request(changes_url, params=params)
            
            if not changes_data.get('values'):
                break
                
            all_changes.extend(changes_data.get('values', []))
            
            if changes_data.get('isLastPage', True):
                break
                
            start = changes_data.get('nextPageStart')
        
        # Extract file paths
        files = []
        for change in all_changes:
            path = change.get('path', {}).get('toString')
            if path:
                files.append({
                    'path': path,
                    'type': change.get('type')  # ADD, MODIFY, DELETE
                })
                
        return files
    
    def get_file_diff(self, from_commit, to_commit, file_path):
        """
        Get the diff for a specific file between two commits.
        
        Args:
            from_commit: Source commit ID
            to_commit: Target commit ID
            file_path: Path to the file
            
        Returns:
            dict: Diff information
        """
        try:
            # Use the compare/diff endpoint for comparing two commits
            url = f"{self.api_base}/compare/diff"
            params = {
                'from': from_commit,
                'to': to_commit,
                'path': file_path
            }
            
            return self.make_request(url, params=params)
        except requests.exceptions.HTTPError as e:
            if hasattr(e, 'response') and e.response.status_code == 404:
                # File not found or binary file
                self.log(f"Could not get diff for file {file_path}")
                return None
            raise
    
    def analyze_file_diff(self, diff):
        """
        Analyze a file diff to extract LOC statistics.
        
        Args:
            diff: Diff information from API
            
        Returns:
            dict: Line statistics
        """
        stats = {
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        }
        
        if not diff:
            return stats
            
        # Process each diff in the response
        for diff_item in diff.get('diffs', []):
            # Process each hunk in the diff
            for hunk in diff_item.get('hunks', []):
                # Process each segment in the hunk
                for segment in hunk.get('segments', []):
                    segment_type = segment.get('type')
                    lines = segment.get('lines', [])
                    
                    if segment_type == 'ADDED':
                        stats['lines_added'] += len(lines)
                        stats['lines_modified'] += len(lines)
                    elif segment_type == 'REMOVED':
                        stats['lines_removed'] += len(lines)
                        stats['lines_modified'] += len(lines)
                    # CONTEXT segments are unchanged lines
        
        return stats
    
    def is_binary_file(self, file_path):
        """Check if a file is likely binary based on extension."""
        extension = Path(file_path).suffix.lower()[1:] if Path(file_path).suffix else ''
        return extension in self.binary_extensions
        
    def analyze_file_change(self, file_info, from_commit, to_commit):
        """
        Analyze changes to a specific file between two commits.
        
        Args:
            file_info: File information (path and type)
            from_commit: Source commit ID
            to_commit: Target commit ID
            
        Returns:
            dict: File analysis results
        """
        path = file_info['path']
        file_type = file_info['type']
        
        # Extract file metadata
        path_obj = Path(path)
        extension = path_obj.suffix.lower()[1:] if path_obj.suffix else 'no_extension'
        directory = str(path_obj.parent).replace('\\', '/')
        
        # Initialize result
        result = {
            'path': path,
            'type': file_type,
            'extension': extension,
            'directory': directory,
            'stats': {
                'lines_added': 0,
                'lines_removed': 0,
                'lines_modified': 0
            }
        }
        
        # For binary files, use simple statistics
        if self.is_binary_file(path):
            if file_type == 'ADD':
                result['stats']['lines_added'] = 1
            elif file_type == 'DELETE':
                result['stats']['lines_removed'] = 1
            elif file_type == 'MODIFY':
                result['stats']['lines_modified'] = 1
        else:
            # For text files, get the diff between commits
            diff = self.get_file_diff(from_commit, to_commit, path)
            if diff:
                result['stats'] = self.analyze_file_diff(diff)
        
        return result
    
    def get_compare_changes(self, from_commit, to_commit):
        """
        Get file changes between two commits using the compare endpoint.
        
        Args:
            from_commit: Source commit ID
            to_commit: Target commit ID
            
        Returns:
            list: List of file changes
        """
        url = f"{self.api_base}/compare/changes"
        params = {
            'from': from_commit,
            'to': to_commit,
            'limit': 1000
        }
        
        all_changes = []
        start = 0
        
        while True:
            params['start'] = start
            response = self.make_request(url, params=params)
            
            if not response.get('values'):
                break
                
            all_changes.extend(response.get('values', []))
            
            if response.get('isLastPage', True):
                break
                
            start = response.get('nextPageStart')
            
        # Convert to our format
        files = []
        for change in all_changes:
            path = change.get('path', {}).get('toString')
            if path:
                files.append({
                    'path': path,
                    'type': change.get('type')  # ADD, MODIFY, DELETE
                })
                
        return files
    
    def analyze_commit_pair(self, current_commit, previous_commit):
        """
        Analyze changes between two commits.
        
        Args:
            current_commit: Current commit to analyze
            previous_commit: Previous commit to compare with (can be None for initial commit)
            
        Returns:
            dict: Analysis results
        """
        current_id = current_commit['id']
        current_short_id = current_id[:8]
        
        # Get the previous commit ID if available
        previous_id = previous_commit['id'] if previous_commit else None
        previous_short_id = previous_id[:8] if previous_id else None
        
        self.log(f"Analyzing commit: {current_short_id}" + 
                (f" (compared to {previous_short_id})" if previous_short_id else ""))
        
        # Initialize commit statistics
        commit_stats = {
            'id': current_id,
            'short_id': current_short_id,
            'author_name': current_commit.get('author', {}).get('name', 'Unknown'),
            'author_email': current_commit.get('author', {}).get('emailAddress', 'Unknown'),
            'date': datetime.fromtimestamp(current_commit.get('authorTimestamp', 0) / 1000).isoformat(),
            'message': current_commit.get('message', ''),
            'compared_with': previous_id,
            'compared_with_short': previous_short_id,
            'files_changed': 0,
            'files_added': 0,
            'files_modified': 0,
            'files_deleted': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0,
            'files': []
        }
        
        # Get file changes between the commits
        if previous_id:
            # If we have two commits to compare
            changed_files = self.get_compare_changes(previous_id, current_id)
        else:
            # For the initial commit, get all files
            changed_files = self.get_commit_files(current_id)
            
        commit_stats['files_changed'] = len(changed_files)
        
        # Counter for file types
        for file_info in changed_files:
            file_type = file_info['type']
            if file_type == 'ADD':
                commit_stats['files_added'] += 1
            elif file_type == 'MODIFY':
                commit_stats['files_modified'] += 1
            elif file_type == 'DELETE':
                commit_stats['files_deleted'] += 1
        
        # Process each file
        for file_info in changed_files:
            file_analysis = self.analyze_file_change(file_info, previous_id, current_id)
            commit_stats['files'].append(file_analysis)
            
            # Update line statistics
            lines_added = file_analysis['stats']['lines_added']
            lines_removed = file_analysis['stats']['lines_removed']
            lines_modified = file_analysis['stats']['lines_modified']
            
            commit_stats['lines_added'] += lines_added
            commit_stats['lines_removed'] += lines_removed
            commit_stats['lines_modified'] += lines_modified
            
            # Update file path stats
            path = file_analysis['path']
            self.file_stats[path]['changes'] += 1
            self.file_stats[path]['lines_added'] += lines_added
            self.file_stats[path]['lines_removed'] += lines_removed
            self.file_stats[path]['lines_modified'] += lines_modified
            
            # Update extension stats
            extension = file_analysis['extension']
            self.extension_stats[extension]['files'] += 1
            self.extension_stats[extension]['lines_added'] += lines_added
            self.extension_stats[extension]['lines_removed'] += lines_removed
            self.extension_stats[extension]['lines_modified'] += lines_modified
            
            # Update directory stats
            directory = file_analysis['directory'] or '/'
            self.directory_stats[directory]['files'] += 1
            self.directory_stats[directory]['lines_added'] += lines_added
            self.directory_stats[directory]['lines_removed'] += lines_removed
            self.directory_stats[directory]['lines_modified'] += lines_modified
        
        # Update author statistics
        author_name = commit_stats['author_name']
        self.author_stats[author_name]['commits'] += 1
        self.author_stats[author_name]['files_changed'] += commit_stats['files_changed']
        self.author_stats[author_name]['files_added'] += commit_stats['files_added']
        self.author_stats[author_name]['files_modified'] += commit_stats['files_modified']
        self.author_stats[author_name]['files_deleted'] += commit_stats['files_deleted']
        self.author_stats[author_name]['lines_added'] += commit_stats['lines_added']
        self.author_stats[author_name]['lines_removed'] += commit_stats['lines_removed']
        self.author_stats[author_name]['lines_modified'] += commit_stats['lines_modified']
        
        # Update total statistics
        self.total_stats['files_changed'] += commit_stats['files_changed']
        self.total_stats['files_added'] += commit_stats['files_added']
        self.total_stats['files_modified'] += commit_stats['files_modified']
        self.total_stats['files_deleted'] += commit_stats['files_deleted']
        self.total_stats['lines_added'] += commit_stats['lines_added']
        self.total_stats['lines_removed'] += commit_stats['lines_removed']
        self.total_stats['lines_modified'] += commit_stats['lines_modified']
        
        # Increment commit counter
        self.commits_analyzed += 1
        
        return commit_stats


def new_analyze_repository(self, from_commit=None, to_commit=None):
    """
    Analyze the repository by moving sequentially through each commit pair in the history.
    
    Args:
        from_commit: Starting commit ID (optional)
        to_commit: Ending commit ID (optional)
        
    Returns:
        dict: Analysis results
    """
    # Get all commits
    all_commits = self.get_all_commits(from_commit, to_commit)
    if not all_commits:
        print("No commits found to analyze.")
        sys.exit(1)
    
    # Create a commit lookup dictionary for faster access
    commit_dict = {commit['id']: commit for commit in all_commits}
    
    # Determine which commits to analyze
    if self.only_significant:
        # Get initial and merge commits
        initial_commit = self.get_initial_commit(all_commits)
        merge_commits = self.get_merge_commits(all_commits)
        
        # Create chronological list of significant commits
        significant_commits = []
        if initial_commit:
            significant_commits.append(initial_commit)
            print(f"Found initial commit: {initial_commit['id'][:8]}")
        
        if merge_commits:
            # Sort merge commits chronologically
            merge_commits.sort(key=lambda x: x.get('authorTimestamp', 0))
            significant_commits.extend(merge_commits)
            print(f"Found {len(merge_commits)} merge commits")
        
        # Filter the commit list to only significant commits
        commits_to_process = significant_commits
        print(f"Analyzing {len(commits_to_process)} significant commits")
    else:
        # Use all commits
        commits_to_process = all_commits
        print(f"Analyzing all {len(commits_to_process)} commits")
    
    # Sort commits by timestamp to ensure chronological order
    commits_to_process.sort(key=lambda x: x.get('authorTimestamp', 0))
    
    # Initialize results
    results = {
        'repository': self.repo,
        'branch': self.branch,
        'analysis_date': datetime.now().isoformat(),
        'commit_range': {
            'from': from_commit,
            'to': to_commit or self.branch
        },
        'total_commits': len(all_commits),
        'analyzed_commits': len(commits_to_process),
        'analyzed_types': 'significant' if self.only_significant else 'all',
        'commits': []
    }
    
    # Reset statistics
    self.commits_analyzed = 0
    
    # Keep track of the last analyzed commit for significant commit mode
    last_analyzed_commit = None
    
    # Process each commit in sequence
    for i, commit in enumerate(commits_to_process):
        commit_id = commit['id']
        short_id = commit_id[:8]
        print(f"Processing commit {i+1}/{len(commits_to_process)}: {short_id}")
        
        # Determine the parent commit to compare against
        if self.only_significant and last_analyzed_commit:
            # For significant commits, compare with the previous significant commit
            compare_commit = last_analyzed_commit
        else:
            # For regular analysis, get the parent commit
            parent_ids = [p.get('id') for p in commit.get('parents', [])]
            if parent_ids:
                # Use the first parent (main branch in case of merge)
                compare_commit = commit_dict.get(parent_ids[0])
            else:
                # Initial commit has no parent
                compare_commit = None
        
        # Analyze the commit pair
        if compare_commit:
            commit_analysis = self.analyze_commit_pair(commit, compare_commit)
        else:
            # For initial commit, compare with empty state
            commit_analysis = self.analyze_commit_pair(commit, None)
        
        results['commits'].append(commit_analysis)
        
        # Update the last analyzed commit
        last_analyzed_commit = commit
    
    # Prepare summary statistics
    results['summary'] = {
        'total': self.total_stats,
        'files': dict(self.file_stats),
        'authors': dict(self.author_stats),
        'extensions': dict(self.extension_stats),
        'directories': dict(self.directory_stats)
    }
    
    print(f"Analysis complete. Processed {self.commits_analyzed} commits.")
    return results


    def analyze_repository(self, from_commit=None, to_commit=None):
        """
        Analyze the repository - either all commits or only significant ones.
        
        Args:
            from_commit: Starting commit ID (optional)
            to_commit: Ending commit ID (optional)
            
        Returns:
            dict: Analysis results
        """
        # Get all commits
        all_commits = self.get_all_commits(from_commit, to_commit)
        if not all_commits:
            print("No commits found to analyze.")
            sys.exit(1)
        
        # Create a commit lookup dictionary
        commit_dict = {commit['id']: commit for commit in all_commits}
        
        # Prepare commit pairs to analyze
        if self.only_significant:
            # For significant commits, get pairs with previous significant commit
            commit_pairs = self.filter_significant_commits(all_commits)
            print(f"Analyzing {len(commit_pairs)} significant commits (initial + merge commits)")
        else:
            # For regular analysis, create pairs with direct parent
            commit_pairs = []
            for commit in all_commits:
                parent_id = commit.get('parents', [{}])[0].get('id') if commit.get('parents') else None
                parent_commit = commit_dict.get(parent_id) if parent_id else None
                commit_pairs.append((commit, parent_commit))
            print(f"Analyzing all {len(commit_pairs)} commits")
            
        # Reset statistics
        self.commits_analyzed = 0
        
        # Initialize results
        results = {
            'repository': self.repo,
            'branch': self.branch,
            'analysis_date': datetime.now().isoformat(),
            'commit_range': {
                'from': from_commit,
                'to': to_commit or self.branch
            },
            'total_commits': len(all_commits),
            'analyzed_commits': len(commit_pairs),
            'analyzed_types': 'significant' if self.only_significant else 'all',
            'commits': []
        }
        
        # Process each commit pair
        for i, (commit, parent_commit) in enumerate(commit_pairs):
            print(f"Processing commit {i+1}/{len(commit_pairs)}: {commit['id'][:8]}")
            commit_analysis = self.analyze_commit_pair(commit, parent_commit)
            results['commits'].append(commit_analysis)
        
        # Prepare summary statistics
        results['summary'] = {
            'total': self.total_stats,
            'files': dict(self.file_stats),
            'authors': dict(self.author_stats),
            'extensions': dict(self.extension_stats),
            'directories': dict(self.directory_stats)
        }
        
        print(f"Analysis complete. Processed {self.commits_analyzed} commits.")
        return results
    
    def export_results(self, results, output_dir='.'):
        """
        Export analysis results to files.
        
        Args:
            results: Analysis results
            output_dir: Output directory
        """
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        repo_name = self.repo.replace('/', '_')
        
        # Export JSON results
        json_path = os.path.join(output_dir, f"{repo_name}_{timestamp}_analysis.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"Full analysis saved to: {json_path}")
        
        # Export author statistics CSV
        authors_csv = os.path.join(output_dir, f"{repo_name}_{timestamp}_authors.csv")
        with open(authors_csv, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['name', 'email', 'commits', 'files_changed', 'files_added', 
                         'files_modified', 'files_deleted', 'lines_added', 
                         'lines_removed', 'lines_modified', 'net_change']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for name, stats in results['summary']['authors'].items():
                # Find email for author
                email = 'Unknown'
                for commit in results['commits']:
                    if commit['author_name'] == name:
                        email = commit['author_email']
                        break
                
                # Calculate net change
                net_change = stats['lines_added'] - stats['lines_removed']
                
                # Write row
                writer.writerow({
                    'name': name,
                    'email': email,
                    'commits': stats['commits'],
                    'files_changed': stats['files_changed'],
                    'files_added': stats['files_added'],
                    'files_modified': stats['files_modified'],
                    'files_deleted': stats['files_deleted'],
                    'lines_added': stats['lines_added'],
                    'lines_removed': stats['lines_removed'],
                    'lines_modified': stats['lines_modified'],
                    'net_change': net_change
                })
        print(f"Author statistics saved to: {authors_csv}")
        
        # Export file statistics CSV
        files_csv = os.path.join(output_dir, f"{repo_name}_{timestamp}_files.csv")
        with open(files_csv, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['path', 'changes', 'lines_added', 'lines_removed', 
                         'lines_modified', 'net_change']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            # Sort files by number of changes
            sorted_files = sorted(results['summary']['files'].items(), 
                                 key=lambda x: x[1]['changes'], reverse=True)
            
            for path, stats in sorted_files:
                writer.writerow({
                    'path': path,
                    'changes': stats['changes'],
                    'lines_added': stats['lines_added'],
                    'lines_removed': stats['lines_removed'],
                    'lines_modified': stats['lines_modified'],
                    'net_change': stats['lines_added'] - stats['lines_removed']
                })
        print(f"File statistics saved to: {files_csv}")


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Analyze file changes in a Bitbucket repository')
    
    parser.add_argument('--repo', required=True, help='Repository in format PROJECT/REPO')
    parser.add_argument('--branch', default='develop', help='Branch to analyze (default: develop)')
    parser.add_argument('--server', default='http://localhost:7990', help='Bitbucket server URL')
    parser.add_argument('--username', help='Bitbucket username')
    parser.add_argument('--password', help='Bitbucket password')
    parser.add_argument('--token', help='Bitbucket access token')
    parser.add_argument('--output', default='.', help='Output directory for results')
    parser.add_argument('--from-commit', help='Starting commit ID for analysis')
    parser.add_argument('--to-commit', help='Ending commit ID for analysis')
    parser.add_argument('--only-significant', action='store_true', 
                       help='Analyze only initial and merge commits')
    parser.add_argument('--verbose', action='store_true', help='Show detailed progress info')
    
    return parser.parse_args()


def main():
    """Main execution function."""
    args = parse_arguments()
    
    # Set up authentication
    auth = None
    if args.username:
        password = args.password or os.environ.get('BITBUCKET_PASSWORD')
        if not password:
            print("Password not provided. Use --password or set BITBUCKET_PASSWORD environment variable")
            sys.exit(1)
        auth = HTTPBasicAuth(args.username, password)
    
    # Create analyzer
    analyzer = BitbucketAnalyzer(
        server=args.server,
        repo=args.repo,
        branch=args.branch,
        auth=auth,
        token=args.token,
        verbose=args.verbose,
        only_significant=args.only_significant
    )
    
    # Run analysis
    results = analyzer.analyze_repository(
        from_commit=args.from_commit,
        to_commit=args.to_commit
    )
    
    # Export results
    analyzer.export_results(results, args.output)
    
    # Print summary statistics
    total = results['summary']['total']
    print("\n=== Analysis Summary ===")
    print(f"Repository: {results['repository']}")
    print(f"Branch: {results['branch']}")
    print(f"Total commits: {results['total_commits']}")
    
    if args.only_significant:
        print(f"Analyzed commits: {results['analyzed_commits']} (initial + merge commits only)")
    else:
        print(f"Analyzed commits: {results['analyzed_commits']} (all commits)")
        
    print(f"Files changed: {total['files_changed']}")
    print(f"  - Files added: {total['files_added']}")
    print(f"  - Files modified: {total['files_modified']}")
    print(f"  - Files deleted: {total['files_deleted']}")
    print(f"Lines of code:")
    print(f"  - Added: {total['lines_added']}")
    print(f"  - Removed: {total['lines_removed']}")
    print(f"  - Modified: {total['lines_modified']}")
    print(f"  - Net change: {total['lines_added'] - total['lines_removed']}")
    
    print("\nTop contributors:")
    authors = [(name, stats['lines_added'] + stats['lines_removed']) 
              for name, stats in results['summary']['authors'].items()]
    for name, changes in sorted(authors, key=lambda x: x[1], reverse=True)[:5]:
        print(f"  {name}: {changes} lines changed")


if __name__ == "__main__":
    main()
