#!/usr/bin/env python3
"""
Bitbucket Change Analysis

A focused script that correctly analyzes file changes and lines of code in each commit
of a Bitbucket on-premise repository. This script uses Bitbucket Server API v1.0 to
extract detailed statistics and produces cumulative results for all users.

Features:
- Accurate file change detection for each commit
- Detailed LOC tracking (added, removed, modified) for each file
- Cumulative statistics across all commits and users
- Option to analyze only initial and merge commits
- CSV export of detailed and summary statistics

Usage:
    python bitbucket_change_analysis.py --repo PROJECT/REPO [options]

Args:
    --repo: Repository in format PROJECT/REPO
    --branch: Branch to analyze (default: develop)
    --server: Bitbucket server URL (default: http://localhost:7990)
    --username: Bitbucket username for Basic Auth
    --password: Bitbucket password
    --token: Bitbucket access token (alternative to username/password)
    --output: Output directory for results (default: current directory)
    --from-commit: Starting commit ID (optional)
    --to-commit: Ending commit ID (optional)
    --only-significant: Analyze only initial and merge commits
    --verbose: Show detailed progress information
"""

import argparse
import csv
import json
import os
import requests
import sys
import time
from collections import defaultdict, Counter
from datetime import datetime
from pathlib import Path
from requests.auth import HTTPBasicAuth


class BitbucketChangeAnalyzer:
    def __init__(self, server, repo, branch='develop', auth=None, token=None, verbose=False, only_significant=False):
        """Initialize the analyzer with connection parameters."""
        self.server = server.rstrip('/')
        self.repo = repo
        self.branch = branch
        self.auth = auth
        self.token = token
        self.verbose = verbose
        self.only_significant = only_significant
        self.headers = {'Accept': 'application/json'}
        
        if token:
            self.headers['Authorization'] = f'Bearer {token}'
        
        # Parse repository information
        try:
            self.project, self.repo_slug = repo.split('/')
        except ValueError:
            raise ValueError("Repository should be in the format PROJECT/REPO")
        
        # Build API base URL
        self.api_base = f"{self.server}/rest/api/1.0/projects/{self.project}/repos/{self.repo_slug}"
        
        # Initialize result containers
        self.commits_analyzed = 0
        self.total_stats = {
            'files_changed': 0,
            'files_added': 0,
            'files_modified': 0,
            'files_deleted': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        }
        self.file_stats = defaultdict(lambda: {
            'changes': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        })
        self.author_stats = defaultdict(lambda: {
            'commits': 0,
            'files_changed': 0,
            'files_added': 0,
            'files_modified': 0,
            'files_deleted': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        })
        self.extension_stats = defaultdict(lambda: {
            'files': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        })
        self.directory_stats = defaultdict(lambda: {
            'files': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        })
        
    def log(self, message):
        """Print log message if verbose mode is enabled."""
        if self.verbose:
            print(message)
    
    def make_request(self, url, method='GET', params=None, data=None):
        """Make an HTTP request to the Bitbucket API with error handling."""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                response = requests.request(
                    method,
                    url,
                    params=params,
                    json=data,
                    headers=self.headers,
                    auth=self.auth,
                    timeout=30  # Add timeout for safety
                )
                response.raise_for_status()
                return response.json()
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)  # Exponential backoff
                    self.log(f"Request failed, retrying in {wait_time}s: {e}")
                    time.sleep(wait_time)
                else:
                    print(f"Error: API request failed after {max_retries} attempts.")
                    if hasattr(e, 'response') and e.response is not None:
                        print(f"Status code: {e.response.status_code}")
                        print(f"Response body: {e.response.text[:500]}")
                    sys.exit(1)
    
    def get_all_commits(self, from_commit=None, to_commit=None):
        """
        Retrieve all commits for the specified branch.
        
        Args:
            from_commit: Starting commit ID (optional)
            to_commit: Ending commit ID (optional)
            
        Returns:
            list: All commit objects
        """
        commits = []
        start = 0
        limit = 100  # Max per page
        
        # Build request parameters
        params = {
            'limit': limit,
            'start': start
        }
        
        if to_commit:
            params['until'] = to_commit
        else:
            params['until'] = self.branch
            
        if from_commit:
            params['since'] = from_commit
            
        print(f"Fetching commits for {self.repo} (branch: {self.branch})...")
        
        while True:
            url = f"{self.api_base}/commits"
            response = self.make_request(url, params=params)
            
            if not response.get('values'):
                break
                
            commits.extend(response['values'])
            
            if response.get('isLastPage', True):
                break
                
            start = response.get('nextPageStart')
            params['start'] = start
            
        # Sort commits chronologically
        commits.sort(key=lambda x: x.get('authorTimestamp', 0))
        
        print(f"Found {len(commits)} commits")
        return commits
        
    def get_initial_commit(self, commits):
        """
        Find the initial commit of the repository.
        
        Args:
            commits: List of commit objects
            
        Returns:
            dict: Initial commit object or None
        """
        if not commits:
            return None
            
        # Sort by timestamp to find the earliest commit
        sorted_commits = sorted(commits, key=lambda x: x.get('authorTimestamp', 0))
        return sorted_commits[0]
        
    def get_merge_commits(self, commits):
        """
        Find all merge commits in the repository.
        
        Args:
            commits: List of commit objects
            
        Returns:
            list: List of merge commit objects
        """
        # Merge commits have more than one parent
        return [commit for commit in commits if len(commit.get('parents', [])) > 1]
        
    def filter_significant_commits(self, commits):
        """
        Filter commits to include only initial and merge commits, and
        establish commit history tracking to analyze changes between significant points.
        
        Args:
            commits: List of all commit objects
            
        Returns:
            list: A list containing tuples of (commit, previous_significant_commit) pairs
        """
        significant_commits = []
        previous_significant = None
        
        # First, get initial commit
        initial_commit = self.get_initial_commit(commits)
        if initial_commit:
            # Initial commit has no previous significant commit
            significant_commits.append((initial_commit, None))
            previous_significant = initial_commit
            print(f"Found initial commit: {initial_commit['id'][:8]}")
        
        # Then find all merge commits
        merge_commits = self.get_merge_commits(commits)
        if merge_commits:
            # Sort merge commits chronologically
            merge_commits.sort(key=lambda x: x.get('authorTimestamp', 0))
            print(f"Found {len(merge_commits)} merge commits")
            
            # Add each merge commit with its previous significant commit reference
            for merge_commit in merge_commits:
                significant_commits.append((merge_commit, previous_significant))
                previous_significant = merge_commit
        
        print(f"Total significant commits: {len(significant_commits)}")
        return significant_commits
    
    def get_commit_changes(self, commit_id, parent_id=None):
        """
        Get all files changed in a commit or between two commits.
        
        Args:
            commit_id: Target commit ID
            parent_id: Parent commit ID (optional)
            
        Returns:
            list: Changed file objects
        """
        # If we have a parent commit ID, use the compare endpoint
        if parent_id:
            url = f"{self.api_base}/compare/changes"
            params = {
                'from': parent_id,
                'to': commit_id,
                'limit': 1000  # Use high limit to get all changes
            }
        else:
            # Otherwise use the commit changes endpoint
            url = f"{self.api_base}/commits/{commit_id}/changes"
            params = {
                'limit': 1000
            }
        
        # Collect all changes with pagination
        all_changes = []
        start = 0
        
        while True:
            params['start'] = start
            response = self.make_request(url, params=params)
            changes = response.get('values', [])
            
            if not changes:
                break
                
            all_changes.extend(changes)
            
            if response.get('isLastPage', True):
                break
                
            start = response.get('nextPageStart')
            
        return all_changes
    
    def get_file_diff(self, commit_id, path, parent_id=None):
        """
        Get the diff for a specific file.
        
        Args:
            commit_id: Target commit ID
            path: File path
            parent_id: Parent commit ID (optional)
            
        Returns:
            dict: Diff information
        """
        try:
            if parent_id:
                # If comparing with parent, use compare endpoint
                url = f"{self.api_base}/compare/diff"
                params = {
                    'from': parent_id,
                    'to': commit_id,
                    'path': path
                }
            else:
                # Otherwise use commit diff endpoint
                url = f"{self.api_base}/commits/{commit_id}/diff/{path}"
                params = {}
                
            return self.make_request(url, params=params)
        except requests.exceptions.HTTPError as e:
            if hasattr(e, 'response') and e.response.status_code == 404:
                # File not found or binary file
                self.log(f"Warning: Could not get diff for file {path}")
                return None
            raise
    
    def analyze_file_diff(self, diff):
        """
        Analyze a file diff to extract LOC statistics.
        
        Args:
            diff: Diff information from API
            
        Returns:
            dict: Line statistics
        """
        stats = {
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0
        }
        
        if not diff:
            return stats
            
        # Process each diff in the response
        for diff_item in diff.get('diffs', []):
            # Process each hunk in the diff
            for hunk in diff_item.get('hunks', []):
                # Process each segment in the hunk
                for segment in hunk.get('segments', []):
                    segment_type = segment.get('type')
                    lines = segment.get('lines', [])
                    
                    if segment_type == 'ADDED':
                        stats['lines_added'] += len(lines)
                        stats['lines_modified'] += len(lines)
                    elif segment_type == 'REMOVED':
                        stats['lines_removed'] += len(lines)
                        stats['lines_modified'] += len(lines)
                    # CONTEXT segments are unchanged lines
        
        return stats
    
    def analyze_file(self, commit_id, file_info, file_type, parent_id=None):
        """
        Analyze a single file change.
        
        Args:
            commit_id: Commit ID
            file_info: File change information from API
            file_type: File change type (ADD, MODIFY, DELETE)
            parent_id: Parent commit ID (optional)
            
        Returns:
            dict: File analysis results
        """
        path = file_info.get('path', {}).get('toString', '')
        if not path:
            return None
            
        # Calculate path components
        path_obj = Path(path)
        extension = path_obj.suffix.lower()[1:] if path_obj.suffix else 'no_extension'
        directory = str(path_obj.parent).replace('\\', '/')
        
        # Initialize result structure
        result = {
            'path': path,
            'type': file_type,
            'extension': extension,
            'directory': directory,
            'stats': {
                'lines_added': 0,
                'lines_removed': 0,
                'lines_modified': 0
            }
        }
        
        # For binary files or directories, use simple statistics
        if self.is_binary_file(path):
            # For binary files, count adds/removes but not line changes
            if file_type == 'ADD':
                result['stats']['lines_added'] = 1
            elif file_type == 'DELETE':
                result['stats']['lines_removed'] = 1
            elif file_type == 'MODIFY':
                result['stats']['lines_modified'] = 1
        else:
            # For text files, get detailed diff
            diff = self.get_file_diff(commit_id, path, parent_id)
            result['stats'] = self.analyze_file_diff(diff)
            
        return result
    
    def is_binary_file(self, path):
        """Check if a file is likely binary based on extension."""
        binary_extensions = {
            'png', 'jpg', 'jpeg', 'gif', 'bmp', 'ico', 'pdf', 'zip', 
            'tar', 'gz', 'jar', 'war', 'ear', 'class', 'exe', 'dll',
            'pdb', 'bin', 'svg', 'woff', 'woff2', 'ttf', 'eot', 'mp3',
            'mp4', 'avi', 'mov', 'wmv', 'flv', 'mkv', 'webm', 'ogg'
        }
        
        extension = Path(path).suffix.lower()[1:] if Path(path).suffix else ''
        return extension in binary_extensions
    
    def analyze_commit(self, commit):
        """
        Analyze a single commit with file-by-file analysis.
        
        Args:
            commit: Commit information from API
            
        Returns:
            dict: Commit analysis results
        """
        commit_id = commit['id']
        short_id = commit_id[:8]
        
        # Get parent commit ID for comparison
        parent_id = None
        if commit.get('parents'):
            parent_id = commit.get('parents', [{}])[0].get('id')
            
        self.log(f"Analyzing commit: {short_id}")
        
        # Get changed files
        changes = self.get_commit_changes(commit_id, parent_id)
        
        # Initialize commit statistics
        commit_stats = {
            'id': commit_id,
            'short_id': short_id,
            'author_name': commit.get('author', {}).get('name', 'Unknown'),
            'author_email': commit.get('author', {}).get('emailAddress', 'Unknown'),
            'date': datetime.fromtimestamp(commit.get('authorTimestamp', 0) / 1000).isoformat(),
            'message': commit.get('message', ''),
            'files_changed': len(changes),
            'files_added': 0,
            'files_modified': 0,
            'files_deleted': 0,
            'lines_added': 0,
            'lines_removed': 0,
            'lines_modified': 0,
            'files': []
        }
        
        # Author name for tracking
        author_name = commit_stats['author_name']
        
        # Update author stats
        self.author_stats[author_name]['commits'] += 1
        
        # Process each file
        for change in changes:
            file_type = change.get('type')
            
            # Update file counts
            if file_type == 'ADD':
                commit_stats['files_added'] += 1
            elif file_type == 'MODIFY':
                commit_stats['files_modified'] += 1
            elif file_type == 'DELETE':
                commit_stats['files_deleted'] += 1
                
            # Analyze file details
            file_analysis = self.analyze_file(commit_id, change, parent_id)
            if not file_analysis:
                continue
                
            commit_stats['files'].append(file_analysis)
            
            # Update line statistics
            lines_added = file_analysis['stats']['lines_added']
            lines_removed = file_analysis['stats']['lines_removed']
            lines_modified = file_analysis['stats']['lines_modified']
            
            commit_stats['lines_added'] += lines_added
            commit_stats['lines_removed'] += lines_removed
            commit_stats['lines_modified'] += lines_modified
            
            # Update file path stats
            path = file_analysis['path']
            self.file_stats[path]['changes'] += 1
            self.file_stats[path]['lines_added'] += lines_added
            self.file_stats[path]['lines_removed'] += lines_removed
            self.file_stats[path]['lines_modified'] += lines_modified
            
            # Update extension stats
            extension = file_analysis['extension']
            self.extension_stats[extension]['files'] += 1
            self.extension_stats[extension]['lines_added'] += lines_added
            self.extension_stats[extension]['lines_removed'] += lines_removed
            self.extension_stats[extension]['lines_modified'] += lines_modified
            
            # Update directory stats
            directory = file_analysis['directory'] or '/'
            self.directory_stats[directory]['files'] += 1
            self.directory_stats[directory]['lines_added'] += lines_added
            self.directory_stats[directory]['lines_removed'] += lines_removed
            self.directory_stats[directory]['lines_modified'] += lines_modified
        
        # Update author statistics
        self.author_stats[author_name]['files_changed'] += commit_stats['files_changed']
        self.author_stats[author_name]['files_added'] += commit_stats['files_added']
        self.author_stats[author_name]['files_modified'] += commit_stats['files_modified']
        self.author_stats[author_name]['files_deleted'] += commit_stats['files_deleted']
        self.author_stats[author_name]['lines_added'] += commit_stats['lines_added']
        self.author_stats[author_name]['lines_removed'] += commit_stats['lines_removed']
        self.author_stats[author_name]['lines_modified'] += commit_stats['lines_modified']
        
        # Update total statistics
        self.total_stats['files_changed'] += commit_stats['files_changed']
        self.total_stats['files_added'] += commit_stats['files_added']
        self.total_stats['files_modified'] += commit_stats['files_modified']
        self.total_stats['files_deleted'] += commit_stats['files_deleted']
        self.total_stats['lines_added'] += commit_stats['lines_added']
        self.total_stats['lines_removed'] += commit_stats['lines_removed']
        self.total_stats['lines_modified'] += commit_stats['lines_modified']
        
        # Increment commit counter
        self.commits_analyzed += 1
        
        return commit_stats
    
    def analyze_repository(self, from_commit=None, to_commit=None):
        """
        Analyze the repository - either all commits or only significant ones.
        
        Args:
            from_commit: Starting commit ID (optional)
            to_commit: Ending commit ID (optional)
            
        Returns:
            dict: Analysis results
        """
        # Get all commits
        all_commits = self.get_all_commits(from_commit, to_commit)
        if not all_commits:
            print("No commits found to analyze.")
            sys.exit(1)
        
        # Filter to significant commits if requested
        if self.only_significant:
            significant_pairs = self.filter_significant_commits(all_commits)
            if not significant_pairs:
                print("No significant commits (initial or merge) found in the repository.")
                sys.exit(1)
            print(f"Analyzing {len(significant_pairs)} significant commits (initial + merge commits)")
            
            # For significant commits, we'll analyze pairs differently
            commits_to_analyze = significant_pairs
            analyzing_pairs = True
        else:
            commits = all_commits
            commits_to_analyze = [(commit, None) for commit in commits]  # Make compatible format
            analyzing_pairs = False
            print(f"Analyzing all {len(commits)} commits")
            
        # Reset statistics
        self.commits_analyzed = 0
        
        # Initialize results structure
        results = {
            'repository': self.repo,
            'branch': self.branch,
            'analysis_date': datetime.now().isoformat(),
            'commit_range': {
                'from': from_commit,
                'to': to_commit or self.branch
            },
            'total_commits': len(all_commits),
            'analyzed_commits': len(commits_to_analyze),
            'analyzed_types': 'significant' if self.only_significant else 'all',
            'commits': []
        }
        
        # Process each commit or commit pair
        for i, (commit, previous_significant) in enumerate(commits_to_analyze):
            if analyzing_pairs and previous_significant:
                print(f"Processing significant commit {i+1}/{len(commits_to_analyze)}: {commit['id'][:8]} " + 
                     f"(comparing with previous: {previous_significant['id'][:8]})")
                # When analyzing significant commits, we want all changes since the previous significant commit
                commit_analysis = self.analyze_commit(commit, comparison_commit=previous_significant)
            else:
                # For regular analysis or the initial commit
                print(f"Processing commit {i+1}/{len(commits_to_analyze)}: {commit['id'][:8]}")
                commit_analysis = self.analyze_commit(commit)
                
            results['commits'].append(commit_analysis)
        
        # Prepare summary statistics
        results['summary'] = {
            'total': self.total_stats,
            'files': dict(self.file_stats),
            'authors': dict(self.author_stats),
            'extensions': dict(self.extension_stats),
            'directories': dict(self.directory_stats)
        }
        
        print(f"Analysis complete. Processed {self.commits_analyzed} commits.")
        return results
    
    def export_results(self, results, output_dir='.'):
        """
        Export analysis results to files.
        
        Args:
            results: Analysis results
            output_dir: Output directory
        """
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        repo_name = self.repo.replace('/', '_')
        
        # Export JSON results
        json_path = os.path.join(output_dir, f"{repo_name}_{timestamp}_analysis.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"Full analysis saved to: {json_path}")
        
        # Export author statistics CSV
        authors_csv = os.path.join(output_dir, f"{repo_name}_{timestamp}_authors.csv")
        with open(authors_csv, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['name', 'email', 'commits', 'files_changed', 'files_added', 
                         'files_modified', 'files_deleted', 'lines_added', 
                         'lines_removed', 'lines_modified', 'net_change']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for name, stats in results['summary']['authors'].items():
                # Find email for author
                email = 'Unknown'
                for commit in results['commits']:
                    if commit['author_name'] == name:
                        email = commit['author_email']
                        break
                
                # Calculate net change
                net_change = stats['lines_added'] - stats['lines_removed']
                
                # Write row
                writer.writerow({
                    'name': name,
                    'email': email,
                    'commits': stats['commits'],
                    'files_changed': stats['files_changed'],
                    'files_added': stats['files_added'],
                    'files_modified': stats['files_modified'],
                    'files_deleted': stats['files_deleted'],
                    'lines_added': stats['lines_added'],
                    'lines_removed': stats['lines_removed'],
                    'lines_modified': stats['lines_modified'],
                    'net_change': net_change
                })
        print(f"Author statistics saved to: {authors_csv}")
        
        # Export cumulative statistics CSV
        stats_csv = os.path.join(output_dir, f"{repo_name}_{timestamp}_stats.csv")
        with open(stats_csv, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['category', 'files_changed', 'files_added', 'files_modified', 
                         'files_deleted', 'lines_added', 'lines_removed', 
                         'lines_modified', 'net_change']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            # Write total stats
            total = results['summary']['total']
            writer.writerow({
                'category': 'Total',
                'files_changed': total['files_changed'],
                'files_added': total['files_added'],
                'files_modified': total['files_modified'],
                'files_deleted': total['files_deleted'],
                'lines_added': total['lines_added'],
                'lines_removed': total['lines_removed'],
                'lines_modified': total['lines_modified'],
                'net_change': total['lines_added'] - total['lines_removed']
            })
            
            # Write extension stats
            for ext, stats in sorted(results['summary']['extensions'].items(), 
                                    key=lambda x: x[1]['files'], reverse=True):
                writer.writerow({
                    'category': f"Extension: {ext}",
                    'files_changed': stats['files'],
                    'files_added': '',  # Not tracked per extension
                    'files_modified': '',
                    'files_deleted': '',
                    'lines_added': stats['lines_added'],
                    'lines_removed': stats['lines_removed'],
                    'lines_modified': stats['lines_modified'],
                    'net_change': stats['lines_added'] - stats['lines_removed']
                })
                
            # Write directory stats
            for directory, stats in sorted(results['summary']['directories'].items(), 
                                         key=lambda x: x[1]['files'], reverse=True):
                writer.writerow({
                    'category': f"Directory: {directory}",
                    'files_changed': stats['files'],
                    'files_added': '',  # Not tracked per directory
                    'files_modified': '',
                    'files_deleted': '',
                    'lines_added': stats['lines_added'],
                    'lines_removed': stats['lines_removed'],
                    'lines_modified': stats['lines_modified'],
                    'net_change': stats['lines_added'] - stats['lines_removed']
                })
        print(f"Overall statistics saved to: {stats_csv}")
        
        # Export most changed files CSV
        files_csv = os.path.join(output_dir, f"{repo_name}_{timestamp}_files.csv")
        with open(files_csv, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['path', 'changes', 'lines_added', 'lines_removed', 
                         'lines_modified', 'net_change']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            # Sort files by number of changes
            sorted_files = sorted(results['summary']['files'].items(), 
                                 key=lambda x: x[1]['changes'], reverse=True)
            
            for path, stats in sorted_files:
                writer.writerow({
                    'path': path,
                    'changes': stats['changes'],
                    'lines_added': stats['lines_added'],
                    'lines_removed': stats['lines_removed'],
                    'lines_modified': stats['lines_modified'],
                    'net_change': stats['lines_added'] - stats['lines_removed']
                })
        print(f"File statistics saved to: {files_csv}")


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Analyze file changes in a Bitbucket repository')
    
    parser.add_argument('--repo', required=True, help='Repository in format PROJECT/REPO')
    parser.add_argument('--branch', default='develop', help='Branch to analyze (default: develop)')
    parser.add_argument('--server', default='http://localhost:7990', help='Bitbucket server URL')
    parser.add_argument('--username', help='Bitbucket username')
    parser.add_argument('--password', help='Bitbucket password')
    parser.add_argument('--token', help='Bitbucket access token')
    parser.add_argument('--output', default='.', help='Output directory for results')
    parser.add_argument('--from-commit', help='Starting commit ID for analysis')
    parser.add_argument('--to-commit', help='Ending commit ID for analysis')
    parser.add_argument('--only-significant', action='store_true', 
                       help='Analyze only initial and merge commits')
    parser.add_argument('--verbose', action='store_true', help='Show detailed progress info')
    
    return parser.parse_args()


def main():
    """Main execution function."""
    args = parse_arguments()
    
    # Set up authentication
    auth = None
    if args.username:
        password = args.password or os.environ.get('BITBUCKET_PASSWORD')
        if not password:
            print("Password not provided. Use --password or set BITBUCKET_PASSWORD environment variable")
            sys.exit(1)
        auth = HTTPBasicAuth(args.username, password)
    
    # Create analyzer
    analyzer = BitbucketChangeAnalyzer(
        server=args.server,
        repo=args.repo,
        branch=args.branch,
        auth=auth,
        token=args.token,
        verbose=args.verbose,
        only_significant=args.only_significant
    )
    
    # Run analysis
    results = analyzer.analyze_repository(
        from_commit=args.from_commit,
        to_commit=args.to_commit
    )
    
    # Export results
    analyzer.export_results(results, args.output)
    
    # Print summary statistics
    total = results['summary']['total']
    print("\n=== Analysis Summary ===")
    print(f"Repository: {results['repository']}")
    print(f"Branch: {results['branch']}")
    print(f"Total commits: {results['total_commits']}")
    
    if args.only_significant:
        print(f"Analyzed commits: {results['analyzed_commits']} (initial + merge commits only)")
    else:
        print(f"Analyzed commits: {results['analyzed_commits']} (all commits)")
        
    print(f"Files changed: {total['files_changed']}")
    print(f"  - Files added: {total['files_added']}")
    print(f"  - Files modified: {total['files_modified']}")
    print(f"  - Files deleted: {total['files_deleted']}")
    print(f"Lines of code:")
    print(f"  - Added: {total['lines_added']}")
    print(f"  - Removed: {total['lines_removed']}")
    print(f"  - Modified: {total['lines_modified']}")
    print(f"  - Net change: {total['lines_added'] - total['lines_removed']}")
    
    print("\nTop contributors:")
    authors = [(name, stats['lines_added'] + stats['lines_removed']) 
              for name, stats in results['summary']['authors'].items()]
    for name, changes in sorted(authors, key=lambda x: x[1], reverse=True)[:5]:
        print(f"  {name}: {changes} lines changed")


if __name__ == "__main__":
    main()
